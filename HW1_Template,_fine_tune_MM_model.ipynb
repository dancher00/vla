{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dancher00/vla/blob/main/HW1_Template%2C_fine_tune_MM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqMsvBANlvDv"
      },
      "source": [
        "## Prerequisites\n",
        "Before we start, make sure you have the following:\n",
        "\n",
        "- Access to a GPU (preferably A100 since videos require high sequence lengths).\n",
        "- Familiarity with Hugging Face’s Transformers library.\n",
        "- Pre-install necessary packages by running the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T8dupbwlvDx",
        "outputId": "3ef6018b-fa1e-4a5f-8961-97e65ef40c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/13.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/13.6 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/13.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/13.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m13.1/13.6 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -U -q transformers accelerate bitsandbytes peft datasets\n",
        "# !pip install -q av decord\n",
        "# !pip install -q lightning\n",
        "# !pip install -q pyarrow==15.0.0\n",
        "# !pip install -q wandb\n",
        "# !pip install -q timm deepspeed flash_attn\n",
        "# restart notebooks here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTG2AJZT89qk"
      },
      "source": [
        "## Fine-tune InternVL 1B. on MMBench dataset\n",
        "\n",
        "In this notebook, you need to fine-tune the [InternVL](https://huggingface.co/OpenGVLab/InternVL3_5-1B) model on [MVBench](https://huggingface.co/datasets/OpenGVLab/MVBench) dataset which is comprised of various video-related tasks. Note that MMBench is quite small and is not made for tuning. So firstly you need to split it into training/testing parts.\n",
        "\n",
        "The goal for the model in this notebook is to answer given multiple choice questions based on the video. The questions can be realetd to temporal aspects of the video, pose prediction and so on.\n",
        "Sources:\n",
        "\n",
        "* InternVL [documentation](https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html)\n",
        "* InternVL [checkpoint on the hub](https://huggingface.co/OpenGVLab/InternVL2-1B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szf17AKL89qm"
      },
      "source": [
        "## Define variables\n",
        "\n",
        "We'll first set some variables useful througout this notebook and doo all the necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJtnWc3b89qn",
        "outputId": "ecdb83b2-e427-4546-9e69-4c733a8300b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import av\n",
        "import re\n",
        "import gc\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import bisect\n",
        "import shutil\n",
        "import traceback\n",
        "import numpy as np\n",
        "from nltk import edit_distance\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "from typing import Dict, Any, List, Union, Tuple\n",
        "\n",
        "from transformers import (AutoConfig, AutoModelForCausalLM, AutoTokenizer,\n",
        "                          HfArgumentParser, Trainer, TrainingArguments,\n",
        "                          set_seed, AutoProcessor)\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from huggingface_hub import snapshot_download, hf_hub_download\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from PIL import Image, ImageFile, PngImagePlugin, UnidentifiedImageError\n",
        "\n",
        "import lightning as L\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping, Callback\n",
        "\n",
        "\n",
        "MAX_LENGTH = 160\n",
        "MODEL_ID = \"OpenGVLab/InternVL3_5-1B\"\n",
        "REPO_ID = \"xxxx\" # Change to your hf-hub repo\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = \"xxxx\" # Change to your W&B profile if you need it\n",
        "os.environ[\"WANDB_MODE\"] = \"online\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "access_token = \"xxxxx\" # Change to your РА profile\n",
        "login(access_token)\n",
        "\n",
        "USE_LORA = False\n",
        "USE_QLORA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8JeFyKyoT4X",
        "outputId": "1a8da900-90e2-4587-c169-9e71f9c66d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'InternVL'...\n",
            "remote: Enumerating objects: 2483, done.\u001b[K\n",
            "remote: Counting objects: 100% (941/941), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 2483 (delta 797), reused 789 (delta 758), pack-reused 1542 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2483/2483), 36.32 MiB | 21.04 MiB/s, done.\n",
            "Resolving deltas: 100% (1522/1522), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/OpenGVLab/InternVL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si7CcgJAoc7S",
        "outputId": "191cf088-b4e2-48f4-fd12-93c9c8033669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval\t     pyproject.toml  zero_stage1_config.json\t   zero_stage3_config_70b.json\n",
            "evaluate.sh  README.md\t     zero_stage2_config.json\t   zero_stage3_config.json\n",
            "examples     shell\t     zero_stage3_config_100b.json\n",
            "internvl     tools\t     zero_stage3_config_34b.json\n"
          ]
        }
      ],
      "source": [
        "! ls InternVL/internvl_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXmhQGtEoT9h"
      },
      "outputs": [],
      "source": [
        "sys.path.append('./InternVL/internvl_chat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhtzSpMpoT-7"
      },
      "outputs": [],
      "source": [
        "from internvl.dist_utils import init_dist\n",
        "# from internvl.model.internlm2.modeling_internlm2 import InternLM2ForCausalLM\n",
        "from internvl.model.internvl_chat import (InternVisionConfig,\n",
        "                                          InternVisionModel,\n",
        "                                          InternVLChatConfig,\n",
        "                                          InternVLChatModel)\n",
        "from internvl.patch import (concat_pad_data_collator,\n",
        "                            replace_llama_rmsnorm_with_fused_rmsnorm,\n",
        "                            replace_train_sampler)\n",
        "from internvl.train.constants import (BOX_END_TOKEN, BOX_START_TOKEN,\n",
        "                                      IMG_CONTEXT_TOKEN, IMG_END_TOKEN,\n",
        "                                      IMG_START_TOKEN, QUAD_END_TOKEN,\n",
        "                                      QUAD_START_TOKEN, REF_END_TOKEN,\n",
        "                                      IMAGENET_MEAN, IMAGENET_STD,\n",
        "                                      REF_START_TOKEN)\n",
        "from internvl.train.dataset import (ConcatDataset, read_frames_decord,\n",
        "                                    WeightedConcatDataset, build_transform,\n",
        "                                    dynamic_preprocess, preprocess,\n",
        "                                    preprocess_internlm, preprocess_mpt,\n",
        "                                    preprocess_phi3,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM0HKJyKEwlp"
      },
      "source": [
        "# MVBench benchmark\n",
        "\n",
        "[MVBench on HF Datasets](https://huggingface.co/datasets/OpenGVLab/MVBench)\n",
        "\n",
        "<!-- ![MVbench1.png](https://huggingface.co/datasets/OpenGVLab/MVBench/resolve/main/assert/generation.png) -->\n",
        "\n",
        "It consists of the 20 temporal task examples as follows.\n",
        "\n",
        "![MVbench-structure.png](https://huggingface.co/datasets/OpenGVLab/MVBench/resolve/main/assert/task_example.png)\n",
        "\n",
        "\n",
        "Here we have a nice viewer for each task:\n",
        "\n",
        "[Dataset viewer](https://huggingface.co/datasets/OpenGVLab/MVBench/viewer/action_sequence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNZQ3imilvDz"
      },
      "source": [
        "We will start by downloading and processing the dataset. Even though MMBench is a small dataset, it still requires **around 1000B to store the videos**, so make sure you have enough free space.\n",
        "\n",
        "First, we will use this mapping to get the datasets because each one is a separate subset in its own folder. Then we need a few helper functions to download videos and process them to fit the model's format (8 frames each video)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIwJovvg9YpI"
      },
      "outputs": [],
      "source": [
        "data_list = {\n",
        "    \"Action Sequence\": (\"action_sequence.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
        "    \"Action Prediction\": (\"action_prediction.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
        "    \"Action Antonym\": (\"action_antonym.json\", \"ssv2_video/\", \"video\", False),\n",
        "    \"Fine-grained Action\": (\"fine_grained_action.json\", \"Moments_in_Time_Raw/videos/\", \"video\", False),\n",
        "    \"Unexpected Action\": (\"unexpected_action.json\", \"FunQA_test/test/\", \"video\", False),\n",
        "    \"Object Existence\": (\"object_existence.json\", \"clevrer/video_validation/\", \"video\", False),\n",
        "    \"Object Interaction\": (\"object_interaction.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
        "    \"Object Shuffle\": (\"object_shuffle.json\", \"perception/videos/\", \"video\", False),\n",
        "    \"Moving Direction\": (\"moving_direction.json\", \"clevrer/video_validation/\", \"video\", False),\n",
        "    \"Action Localization\": (\"action_localization.json\", \"sta/sta_video/\", \"video\", True),  # has start & end\n",
        "    \"Scene Transition\": (\"scene_transition.json\", \"scene_qa/video/\", \"video\", False),\n",
        "    \"Action Count\": (\"action_count.json\", \"perception/videos/\", \"video\", False),\n",
        "    \"Moving Count\": (\"moving_count.json\", \"clevrer/video_validation/\", \"video\", False),\n",
        "    \"Moving Attribute\": (\"moving_attribute.json\", \"clevrer/video_validation/\", \"video\", False),\n",
        "    \"State Change\": (\"state_change.json\", \"perception/videos/\", \"video\", False),\n",
        "    \"Fine-grained Pose\": (\"fine_grained_pose.json\", \"nturgbd/\", \"video\", False),\n",
        "    \"Character Order\": (\"character_order.json\", \"perception/videos/\", \"video\", False),\n",
        "    \"Egocentric Navigation\": (\"egocentric_navigation.json\", \"vlnqa/\", \"video\", False),\n",
        "    \"Episodic Reasoning\": (\"episodic_reasoning.json\", \"tvqa/frames_fps3_hq/\", \"frame\", True),  # has start & end, read frame\n",
        "    \"Counterfactual Inference\": (\"counterfactual_inference.json\", \"clevrer/video_validation/\", \"video\", False),\n",
        "}\n",
        "\n",
        "data_dir = \"dataset\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(\"dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVdfSXMGlvDz"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(video_path, start, end, n_frames=8):\n",
        "    \"\"\"\n",
        "    Reads a video for given start-end timestamps interval\n",
        "    and uniformly samples 8 frames of it\n",
        "    \"\"\"\n",
        "    container = av.open(video_path)\n",
        "    video = container.streams.get(0)[0]\n",
        "\n",
        "    av_timestamps = [\n",
        "        int(packet.pts * video.time_base) for packet in container.demux(video) if packet.pts is not None\n",
        "    ]\n",
        "\n",
        "    av_timestamps.sort()\n",
        "    start_id = bisect.bisect_left(av_timestamps, start)\n",
        "    end_id = bisect.bisect_left(av_timestamps, end)\n",
        "\n",
        "    # in case it is a very short video, lets take a longer duration and sample\n",
        "    if end_id  - start_id < 10:\n",
        "        end_id += 10\n",
        "        start_id -= 10\n",
        "\n",
        "    end_id = min(len(av_timestamps) - 1, end_id)\n",
        "    start_id = max(1, start_id)\n",
        "\n",
        "    # We sample n_frames frames for tuning following the original paper\n",
        "    # But we can increase the number of frames for longer videos and check out if it helps performance\n",
        "    # Change the below \"n_frames\" to any number of frames you want, and note that more frames -> more computational resources needed\n",
        "    indices = np.linspace(start_id, end_id, n_frames).astype(int)\n",
        "\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_id:\n",
        "            break\n",
        "        if i >= start_id and i in indices:\n",
        "            frames.append(frame)\n",
        "    assert len(frames) == n_frames, f\"Got {len(frames)} frames but should be {n_frames}. Check the indices: {indices};, start_id: {start_id}, end_id: {end_id}. Len of video is {len(av_timestamps)} frames.\"\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9-8O207lvD0"
      },
      "outputs": [],
      "source": [
        "def collate_read_video(example, path):\n",
        "    # Some datasets have a start-end interval, so we try to get it if exists.\n",
        "    # Otherwise just set a very large end timestamp\n",
        "    clip = read_video_pyav(f'{path}/{example[\"video\"]}', example.get(\"start\", 1), example.get(\"end\", 1e+10))\n",
        "    example[\"clip\"] = clip\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jUicyg46_7K"
      },
      "outputs": [],
      "source": [
        "# Download the videos from datasets repo and unzip.\n",
        "# Make sure you have enough free space before downloading and unzipping\n",
        "\n",
        "# videos = snapshot_download(repo_id=\"OpenGVLab/MVBench\", allow_patterns=\"*\", repo_type=\"dataset\")\n",
        "# for zip_file in os.listdir(f\"{videos}/video\"):\n",
        "#     if zip_file.endswith(\".zip\"):\n",
        "#         shutil.unpack_archive(f\"{videos}/video/{zip_file}\", f\"{videos}/videos_unzipped/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrZnpKaflNQG",
        "outputId": "3d52c667-dbbd-49bc-a4e6-9fa11860b8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task: Action Sequence\n",
            "Annotation file: action_sequence.json\n",
            "Videos are stored in: star/Charades_v1_480/\n",
            "Videos are represented as: video\n",
            "Videos are have start/end timestamps: True\n",
            "star.zip\n"
          ]
        }
      ],
      "source": [
        "# Or download only selected task with appropriate files\n",
        "# https://huggingface.co/docs/huggingface_hub/v0.24.7/en/package_reference/file_download#huggingface_hub.hf_hub_download\n",
        "\n",
        "TASK_NAME = \"Action Sequence\"\n",
        "annotation_fn, video_dir, video_type, has_clip = data_list.get(TASK_NAME)\n",
        "print(f\"Task: {TASK_NAME}\")\n",
        "print(f\"Annotation file: {annotation_fn}\")\n",
        "print(f\"Videos are stored in: {video_dir}\")\n",
        "print(f\"Videos are represented as: {video_type}\")\n",
        "print(f\"Videos are have start/end timestamps: {has_clip}\")\n",
        "\n",
        "annotation_fn_local = hf_hub_download(repo_id=\"OpenGVLab/MVBench\",\n",
        "                                    filename='json/' + annotation_fn,\n",
        "                                    repo_type=\"dataset\",\n",
        "                                    local_dir=data_dir)\n",
        "videos_zip = hf_hub_download(repo_id=\"OpenGVLab/MVBench\",\n",
        "                            filename='video/' + video_dir.split(\"/\")[0] + \".zip\",\n",
        "                            repo_type=\"dataset\",\n",
        "                            local_dir=data_dir)\n",
        "\n",
        "# Unzip\n",
        "for zip_file in os.listdir(f\"{data_dir}/video\"):\n",
        "    if zip_file.endswith(\".zip\"):\n",
        "        print(zip_file)\n",
        "        shutil.unpack_archive(f\"{data_dir}/video/{zip_file}\", f\"{data_dir}/video/videos_unzipped/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThTV0DbDA9RP"
      },
      "source": [
        "Make a following data structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "    /json\n",
        "        task_name.json\n",
        "    /video\n",
        "        /task_name_prefix (optional)\n",
        "            /task_name\n",
        "                video_0.mp4\n",
        "                video_1.mp4\n",
        "                video_2.mp4\n",
        "                ...\n",
        "                video_100.mp4\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMNQND-ilvD0",
        "outputId": "ac1c78fc-6fcf-4e4c-b82c-76f53dcfc1eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['video', 'question', 'candidates', 'answer', 'start', 'end'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "ds = load_dataset(\"json\", data_files=annotation_fn_local, split=\"train\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSWorVK8B9ap",
        "outputId": "c0c865e8-9399-4d27-b562-8b9efbf08990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video `EDXBD.mp4` does not exists!\n",
            "Video `K47J5.mp4` does not exists!\n",
            "Video `9MNZ5.mp4` does not exists!\n",
            "Video `QXT9W.mp4` does not exists!\n",
            "Video `ABHC6.mp4` does not exists!\n",
            "Video `ALXUC.mp4` does not exists!\n",
            "Video `BAUQE.mp4` does not exists!\n",
            "Video `PHH6B.mp4` does not exists!\n",
            "Video `MNC10.mp4` does not exists!\n",
            "Video `W7CR5.mp4` does not exists!\n",
            "Video `Q8UJ8.mp4` does not exists!\n",
            "Video `X9WTR.mp4` does not exists!\n"
          ]
        }
      ],
      "source": [
        "# Some tasks in MVBench are missing video files - keep it in mind!\n",
        "has_missing = False\n",
        "for sample in ds:\n",
        "    if not os.path.exists(f\"{data_dir}/video/videos_unzipped/{video_dir}/{sample['video']}\"):\n",
        "        print(f\"Video `{sample['video']}` does not exists!\")\n",
        "        has_missing = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1stzqVuCtRF",
        "outputId": "c11388de-691f-4816-98e7-07b8a2ecdfa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length = 200\n",
            "Dataset length = 188\n"
          ]
        }
      ],
      "source": [
        "print(f\"Dataset length = {len(ds)}\")\n",
        "if has_missing:\n",
        "    ds = ds.filter(lambda x: os.path.exists(f\"{data_dir}/video/videos_unzipped/{video_dir}/{x['video']}\"))\n",
        "\n",
        "print(f\"Dataset length = {len(ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c44886c523194321a45ac116a99d7e09",
            "4c53a11f8b4d48bab7bb07005e09dab2",
            "1e40a2be14574c9cab05c6a2066506c9",
            "6997d1b4ec2e40268bbfe56ff5ad1ddc",
            "0c80b263628d4c37b129278c8cb80fc2",
            "606c508016d34f1bb86d21b2c4e3cfc3",
            "8c9c50dc1265410e82248451e1fdd156",
            "e9e922c6bf14487599e94963b8afd747",
            "e7dd46cf21684684b12361c82c5e01ca",
            "7c13ed62e28e44468633fdfa010a07cb",
            "24b4a2b91c7e484bacb5a543a60dff44"
          ]
        },
        "id": "eySn7X6zlbn3",
        "outputId": "0c291bd5-d7c5-4337-ef61-99d9399d3cad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/188 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c44886c523194321a45ac116a99d7e09"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load videos and split them into frames\n",
        "# ds = ds.map(collate_read_video,\n",
        "#             batched=False,\n",
        "#             fn_kwargs={\"path\": f\"{data_dir}/video/videos_unzipped/{video_dir}\"})\n",
        "\n",
        "# Make conversation\n",
        "\n",
        "def make_conversation(sample):\n",
        "    id2choice = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "    question, candidates = sample[\"question\"], sample[\"candidates\"]\n",
        "    answer_i = candidates.index(sample[\"answer\"])\n",
        "    answer_choice = id2choice[answer_i]\n",
        "\n",
        "    mult_choice = \"\\n\"\n",
        "    for i, choice in enumerate(candidates):\n",
        "        mult_choice += f\"{id2choice[i]}. {choice};\\n\"\n",
        "\n",
        "    conversations = [\n",
        "         {'from': 'human', 'value': question + mult_choice + '<video>'},\n",
        "         {'from': 'gpt', 'value': answer_choice + \" \" + sample[\"answer\"]}\n",
        "    ]\n",
        "    sample['conversations'] = conversations\n",
        "    return sample\n",
        "\n",
        "ds = ds.map(make_conversation,\n",
        "            batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwVmDv2dsMqA",
        "outputId": "98874a15-435d-40fb-f929-765fc303223f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'video': 'ZS9XR.mp4',\n",
              " 'question': 'What happened after the person took the food?',\n",
              " 'candidates': ['Ate the medicine.',\n",
              "  'Tidied up the blanket.',\n",
              "  'Put down the cup/glass/bottle.',\n",
              "  'Took the box.'],\n",
              " 'answer': 'Ate the medicine.',\n",
              " 'start': 1.5,\n",
              " 'end': 17.1,\n",
              " 'conversations': [{'from': 'human',\n",
              "   'value': 'What happened after the person took the food?\\nA. Ate the medicine.;\\nB. Tidied up the blanket.;\\nC. Put down the cup/glass/bottle.;\\nD. Took the box.;\\n<video>'},\n",
              "  {'from': 'gpt', 'value': 'A Ate the medicine.'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```json\n",
        "{'id': 578004, 'video': '013901_013950/1025449886.mp4',\n",
        "'conversations':\n",
        "[{'from': 'human', 'value': 'Render a clear and concise summary of the video below.\\n<video>'},\n",
        "{'from': 'gpt', 'value': 'Aerial; drone flight around dangerous eroded steep stony slopes of cabo da roca; granite boulders, sea cliffs along the high coast; long seashore with lighthouse, overlooking the promontory, portugal'}]\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "c6JsAJjJx1C4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRTrOYp6lvD0",
        "outputId": "e0f038f5-bef8-4695-95a3-4efa5e636e1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2TokenizerFast(name_or_path='OpenGVLab/InternVL2-1B', vocab_size=151643, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
              "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151646: AddedToken(\"<img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151647: AddedToken(\"</img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151648: AddedToken(\"<IMG_CONTEXT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151649: AddedToken(\"<quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151650: AddedToken(\"</quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151651: AddedToken(\"<ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151652: AddedToken(\"</ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151653: AddedToken(\"<box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151654: AddedToken(\"</box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load model's processor\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "processor.padding_side = \"right\"\n",
        "\n",
        "processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJjH0eocsK8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtxYp7h2lvD0"
      },
      "source": [
        "## Custom Dataset Class\n",
        "\n",
        "In the next step, you'll need **to define a custom dataset** class and the necessary functions to prepare our data for fine-tuning model. The VideoQADataset class extends the [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class to facilitate loading and processing \"MMBench\". This class will handle the conversion of dataset samples into the format required for training and evaluation by preparing a prompt and making array from videos.\n",
        "\n",
        "Next, you need **to define collate functions** to handle the batching of data during training and evaluation. These functions ensure that the input data is properly formatted and padded.\n",
        "\n",
        "Here use the processor to turn the (video, target token sequence) into the format that the model expects (which is pixel_values, input_ids etc.). Use a dynamic padding of the batches: each batch contains ground truth sequences of varying lengths.\n",
        "\n",
        "Also you can limit the length of the text tokens (input_ids) to a max length due to memory constraints, feel free to expand if your target token sequences are longer (I'd recommend plotting the average token length of your dataset to determine the optimal value).\n",
        "\n",
        "The formatting of the input_ids is super important: you need to respect a so-called [chat template](https://huggingface.co/docs/transformers/main/en/chat_templating).\n",
        "\n",
        "Labels are created for the model by simply copying the inputs to the LLM (input_ids), but with padding tokens replaced by the ignore index of the loss function. This ensures that the model doesn't need to learn to predict padding tokens (used to batch examples together).\n",
        "\n",
        "Why are the labels a copy of the model inputs, you may ask? The model will internally shift the labels one position to the right so that the model will learn to predict the next token. This can be seen here.\n",
        "\n",
        "The collate function for evaluation is different, since there you only need to feed the prompt to the model, as we'll use the `generate()` method to autoregressively generate a completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un_H3-pPlvD0"
      },
      "outputs": [],
      "source": [
        "class VideoQADataset(Dataset):\n",
        "    \"\"\"Dataset for Video QA fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        template_name,\n",
        "        raw_data,\n",
        "        video_data_dir,\n",
        "        tokenizer,\n",
        "        ds_name,\n",
        "        num_image_token,\n",
        "        image_size=224,\n",
        "        is_train=True,\n",
        "        pad2square=False,\n",
        "        dynamic_image_size=False,\n",
        "        use_thumbnail=False,\n",
        "        min_dynamic_patch=1,\n",
        "        max_dynamic_patch=6,\n",
        "        min_num_frame=4,  # for video data\n",
        "        max_num_frame=12,  # for video data\n",
        "        sampling_method='rand',  # for video data\n",
        "        repeat_time=1,\n",
        "        normalize_type='imagenet',\n",
        "        random_seed=0,\n",
        "    ):\n",
        "        super(VideoQADataset, self).__init__()\n",
        "        self.ds_name = ds_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.template_name = template_name\n",
        "        self.num_image_token = num_image_token\n",
        "        print(f'[Dataset] num_image_token: {num_image_token}')\n",
        "        print(f'[Dataset] dynamic_image_size: {dynamic_image_size}')\n",
        "        print(f'[Dataset] use_thumbnail: {use_thumbnail}')\n",
        "        print(f'[Dataset] min_dynamic_patch: {min_dynamic_patch}, max_dynamic_patch: {max_dynamic_patch}')\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.is_train = is_train\n",
        "        self.pad2square = pad2square\n",
        "        self.max_num_frame = max_num_frame\n",
        "        self.min_num_frame = min_num_frame\n",
        "        self.sampling_method = sampling_method\n",
        "\n",
        "        print('Formatting inputs...Skip in lazy mode')\n",
        "\n",
        "        self.raw_data = raw_data\n",
        "        self.rng = np.random.default_rng(seed=random_seed)\n",
        "        self.rng.shuffle(self.raw_data)\n",
        "\n",
        "        gc.collect()\n",
        "        self.root = video_data_dir\n",
        "        self.cached_data_dict = {}\n",
        "        self.dynamic_image_size = dynamic_image_size\n",
        "        self.use_thumbnail = use_thumbnail\n",
        "        self.min_dynamic_patch = min_dynamic_patch\n",
        "        self.max_dynamic_patch = max_dynamic_patch\n",
        "        self.normalize_type = normalize_type\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def get_preprocess_function(self):\n",
        "        # Select the appropriate preprocessing function based on the template name\n",
        "        return preprocess_internlm\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        # Load the image using tcs_loader if available, otherwise use PIL\n",
        "        if self.tcs_loader is not None and 's3://' in image_path:\n",
        "            return self.tcs_loader(image_path)\n",
        "        return Image.open(image_path).convert('RGB')\n",
        "\n",
        "    def get_image_path(self, image_path):\n",
        "        if image_path.startswith('s3://'):  # for ceph\n",
        "            image_path = self.root + image_path\n",
        "        else:  # for local image\n",
        "            image_path = os.path.join(self.root, image_path)\n",
        "        return image_path\n",
        "\n",
        "    def get_transform(self):\n",
        "        # Build transformation function\n",
        "        transform = build_transform(is_train=self.is_train, input_size=self.image_size,\n",
        "                                    pad2square=self.pad2square, normalize_type=self.normalize_type)\n",
        "        return transform\n",
        "\n",
        "\n",
        "    def video_get_item(self, data_item):\n",
        "        # Build transformation function\n",
        "        transform = self.get_transform()\n",
        "\n",
        "        # Ensure the first conversation contains a video placeholder\n",
        "        if '<video>' not in data_item['conversations'][0]['value']:\n",
        "            data_item['conversations'][0]['value'] = '<video>\\n' + data_item['conversations'][0]['value']\n",
        "\n",
        "        # Get the video file path\n",
        "        video_file = data_item['video']\n",
        "        video_path = os.path.join(self.root, video_file)\n",
        "\n",
        "        # Load the video frames using tcs_loader\n",
        "        image_list = read_frames_decord(video_path,\n",
        "                                        num_frames=self.num_frames,\n",
        "                                        client=self.tcs_loader,\n",
        "                                        clip=(data_item.get('start', None),\n",
        "                                              data_item.get('end', None)))\n",
        "\n",
        "        # Generate special tokens for each video frame\n",
        "        special_tokens = '\\n'.join(['Frame{}: <image>'.format(i + 1) for i in range(len(image_list))])\n",
        "        data_item['conversations'][0]['value'] = data_item['conversations'][0]['value'].replace(\n",
        "            '<video>', special_tokens)\n",
        "\n",
        "        # Transform each frame image and stack them into a tensor\n",
        "        pixel_values = [transform(image) for image in image_list]\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        num_patches = pixel_values.size(0)\n",
        "\n",
        "        # Select the appropriate preprocessing function based on the template name\n",
        "        preprocess_function = self.get_preprocess_function()\n",
        "\n",
        "        # Preprocess the conversations and generate the return dictionary\n",
        "        num_image_tokens = [self.num_image_token] * num_patches\n",
        "        ret = preprocess_function(self.template_name, [deepcopy(data_item['conversations'])],\n",
        "                                  self.tokenizer, num_image_tokens, group_by_length=self.group_by_length,\n",
        "                                  ds_name=self.ds_name, num_image=num_patches)\n",
        "\n",
        "        # Create the final return dictionary\n",
        "        ret = dict(\n",
        "            input_ids=ret['input_ids'][0],\n",
        "            labels=ret['labels'][0],\n",
        "            attention_mask=ret['attention_mask'][0],\n",
        "            pixel_values=pixel_values,\n",
        "            image_flags=torch.tensor([1] * num_patches, dtype=torch.long)\n",
        "        )\n",
        "        return ret\n",
        "\n",
        "    def pure_text_get_item(self, data_item):\n",
        "        # Build transformation function\n",
        "        transform = self.get_transform()\n",
        "\n",
        "        # Create a blank white image\n",
        "        image = Image.new('RGB', (224, 224), (255, 255, 255))\n",
        "\n",
        "        # Dynamically preprocess the image to generate patches\n",
        "        images = dynamic_preprocess(image, min_num=self.min_dynamic_patch, max_num=1,\n",
        "                                    image_size=self.image_size, use_thumbnail=self.use_thumbnail)\n",
        "\n",
        "        # Apply the transformation to each image patch and stack them into a tensor\n",
        "        pixel_values = [transform(image) for image in images]\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        num_patches = pixel_values.size(0)\n",
        "\n",
        "        # Ensure there is only one patch\n",
        "        assert num_patches == 1, f'The number of patches should be 1, but got {num_patches}.'\n",
        "\n",
        "        # Select the appropriate preprocessing function based on the template name\n",
        "        preprocess_function = self.get_preprocess_function()\n",
        "\n",
        "        # Preprocess the conversations and generate the return dictionary\n",
        "        ret = preprocess_function(self.template_name, [deepcopy(data_item['conversations'])],\n",
        "                                  self.tokenizer, [self.num_image_token * num_patches], text_only=True,\n",
        "                                  group_by_length=self.group_by_length, ds_name=self.ds_name)\n",
        "\n",
        "        # Create the final return dictionary\n",
        "        ret = dict(\n",
        "            input_ids=ret['input_ids'][0],\n",
        "            labels=ret['labels'][0],\n",
        "            attention_mask=ret['attention_mask'][0],\n",
        "            pixel_values=pixel_values,\n",
        "            image_flags=torch.tensor([0] * num_patches, dtype=torch.long)\n",
        "        )\n",
        "        return ret\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        i = i % len(self.raw_data)\n",
        "        while True:\n",
        "            try:\n",
        "                data_item = json.loads(self.raw_data[i])\n",
        "                if 'image' in data_item and len(data_item['image']) != 0:\n",
        "                    if type(data_item['image']) == list:\n",
        "                        ret = self.multi_modal_multi_image_get_item(data_item)\n",
        "                    else:\n",
        "                        ret = self.multi_modal_get_item(data_item)\n",
        "                elif 'video' in data_item and data_item['video'] is not None and data_item['video'] != '':\n",
        "                    ret = self.video_get_item(data_item)\n",
        "                else:\n",
        "                    ret = self.pure_text_get_item(data_item)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e, self.ds_name, flush=True)\n",
        "                if not isinstance(e, UnidentifiedImageError):\n",
        "                    traceback.print_exc()\n",
        "                data_item = json.loads(self.raw_data[i])\n",
        "                if 'image' in data_item:\n",
        "                    if type(data_item['image']) == list:\n",
        "                        images = [self.root + item for item in data_item['image']]\n",
        "                        print(f'Failed to load image: {images}, the dataset is: {self.ds_name}')\n",
        "                    else:\n",
        "                        if data_item['image'].startswith('s3://'):\n",
        "                            data_path = self.root + data_item['image']\n",
        "                        else:\n",
        "                            data_path = os.path.join(self.root, data_item['image'])\n",
        "                        print(f'Failed to load image: {data_path}, the dataset is: {self.ds_name}')\n",
        "                elif 'video' in data_item:\n",
        "                    data_path = os.path.join(self.root, data_item['video'])\n",
        "                    print(f'Failed to load video: {data_path}, the dataset is: {self.ds_name}')\n",
        "                i = random.randint(0, len(self.raw_data) - 1)\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ6rlxudlvD1"
      },
      "outputs": [],
      "source": [
        "def train_collate_fn(examples):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return input_ids, attention_mask, pixel_values_videos, labels\n",
        "\n",
        "\n",
        "def eval_collate_fn(examples):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return input_ids, attention_mask, pixel_values_videos, answer_choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sH2oWArlvD1"
      },
      "source": [
        "## Shuffling and Splitting the Dataset\n",
        "You need to shuffle dataset, and then split it into training and test sets. This ensures that our model is trained on a diverse and representative sample of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DenEK2IqlvD1"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_DDgpOulvD1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "example = dataset['train'][0]\n",
        "clip = example[\"clip\"]\n",
        "# np array with shape (frames, height, width, channels)\n",
        "video = np.array(clip)\n",
        "\n",
        "\n",
        "# Vusualize your data\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLtCWizwlvD1"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "im = plt.imshow(video[0,:,:,:])\n",
        "\n",
        "plt.close() # this is required to not display the generated image\n",
        "\n",
        "def init():\n",
        "    im.set_data(video[0,:,:,:])\n",
        "\n",
        "def animate(i):\n",
        "    im.set_data(video[i,:,:,:])\n",
        "    return im\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
        "                               interval=100)\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsc4WadilvD1"
      },
      "source": [
        "And now wrap it in the Pytorch Datasets class and print one example as sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXPeOA34lvD1"
      },
      "outputs": [],
      "source": [
        "train_dataset = VideoLlavaDataset(dataset[\"train\"])\n",
        "eval_dataset = VideoLlavaDataset(dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeI7LaF4lvD1"
      },
      "outputs": [],
      "source": [
        "prompt, clip = train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Th3HTV6A_89"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd4mrynzlvD2"
      },
      "source": [
        "## Load model\n",
        "Next, load your InternVL model from the hub. This is a model with about 1 billion trainable parameters (as it combines a **Qwen2 1B language model** with a relatively low-parameter vision **InternViT encoder**). Do note that we load a model here which already has undergone supervised fine-tuning (SFT) instructions dataset. We can benefit from the fine-tuning that the model already has undergone.\n",
        "\n",
        "## Full fine-tuning, LoRa and Q-LoRa\n",
        "\n",
        "**Select the fine-tuning method.**\n",
        "\n",
        " For reference, fine-tuning a model using the AdamW optimizer (which is often used to optimize neural networks) with mixed precision, you need about 18 times the amount of parameters in GB of GPU RAM. So in this case, we would need 18x1 billion bytes = 18 GB of GPU RAM if we want to update all the parameters of the model. Not so huge right? But using PEFT approach it could be less.\n",
        "\n",
        "Some clever people came up with the LoRa method (LoRa is short for low-rank adapation). It allows to just freeze the existing weights and only train a couple of adapter layers on top of the base model. Hugging Face offers the separate [PEFT library](https://huggingface.co/docs/peft/main/en/index) for easy use of LoRa, along with other Parameter-Efficient Fine-Tuning methods.\n",
        "\n",
        "Moreover, one can not only freeze the existing base model but also quantize it (which means, shrinking down its size). A neural network's parameters are typically saved in either float32 (which means, 32 bits or 4 bytes are used to store each parameter value) or float16 (which means, 16 bits or half a byte - also called half precision). However, with some clever algorithms one can shrink each parameter to just 8 or 4 bits (half a byte!), without significant effect on final performance. Read all about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes.\n",
        "\n",
        "This means that we're going to shrink the size of the base 1B model considerably using 4-bit quantization, and then only train a couple of adapter layers on top using LoRa (in float16). This idea of combining LoRa with quantization is called Q-LoRa and is the most memory friendly version.\n",
        "\n",
        "There exist many forms of quantization, here we leverage the [BitsAndBytes integration](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ0nTqbVlvD2"
      },
      "outputs": [],
      "source": [
        "## Load model\n",
        "# Three options for training, from the lowest precision training to the highest precision training:\n",
        "# QLoRA: model uses 4-bit quantization, which helps in reducing memory usage while maintaining performance.\n",
        "# Standard LoRA:  model is loaded with standard LoRA adaptations.\n",
        "# Full Fine-Tuning: no memory optimization are done. In that case Flash Attention is used to speed up training, if hardware supports it.\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYDW50LslvD2",
        "outputId": "400cf245-42be-42d6-83f1-23211f77cb3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['up_proj', 'down_proj', 'q_proj', 'o_proj', 'k_proj', 'gate_proj', 'v_proj']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def find_all_linear_names(model):\n",
        "    # Only for LoRA ot QLoRA\n",
        "\n",
        "    cls = torch.nn.Linear\n",
        "    lora_module_names = set()\n",
        "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
        "    for name, module in model.named_modules():\n",
        "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
        "            continue\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "# If you selected LoRA ot QLora make a choise of parameters to replace\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Then create LoraConfig and run prepare_model_for_kbit_training(...)\n",
        "# and finally: model = get_peft_model(model, ...)\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuv04h-DlvD2"
      },
      "source": [
        "## Define PyTorch Lightning Module for Video-LLaVA\n",
        "To streamline the training and evaluation of the Video-InternVL model, you can use [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html), which abstracts away much of the boilerplate code and provides a structured framework for model training. In this section, you need to define the InternVLModelPLModule, a custom PyTorch Lightning module that encapsulates the model, training loop, validation loop, and optimizer configuration.\n",
        "\n",
        "### InternVLModelPLModule Class\n",
        "\n",
        "The InternVLModelPLModule class inherits from LightningModule and includes methods for training, validation, and optimizer configuration. This setup ensures a clean and efficient training process.\n",
        "\n",
        "Basically, PyTorch Lightning will take care of all device placements (.to(device)) for us, as well as the backward pass, putting the model in training mode, etc.\n",
        "\n",
        "Notice the difference between a training step and an evaluation step:\n",
        "\n",
        "- a training step only consists of a forward pass, in which we compute the cross-entropy loss between the model's next token predictions and the ground truth (in parallel for all tokens, this technique is known as \"teacher forcing\"). The backward pass is handled by PyTorch Lightning.\n",
        "- an evaluation step consists of making the model autoregressively complete the prompt using the generate() method. After that, you compute an evaluation metric between the predicted sequences and the ground truth ones. This allows you to see how the model is improving over the course of training. The metric we use here is accuracy of answering the question.\n",
        "\n",
        "Besides that, you define the optimizer to use (AdamW is a good default choice) and the data loaders, which use the collate functions defined above to batch together items of the PyTorch datasets. Do note that AdamW is a pretty heavy optimizer in terms of memory requirements, but as we're training with QLoRa we only need to store optimizer states for the adapter layers. For full fine-tuning, one could take a look at more memory friendly optimizers such as 8-bit Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvNdvJ1ylvD2"
      },
      "outputs": [],
      "source": [
        "class InternVLModelPLModule(L.LightningModule):\n",
        "    def __init__(self, config, processor, model):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset, collate_fn=train_collate_fn,\n",
        "                          batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(eval_dataset, collate_fn=eval_collate_fn,\n",
        "                          batch_size=self.batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6iWFbnQlvD2"
      },
      "source": [
        "Then instantiate it (based on a config dictionary which defines all hyperparameters for training).\n",
        "\n",
        "The batch size was determined based on the compute available.\n",
        "\n",
        "Do note that one can play around with the hyperparameters, I just use good defaults here: 10 epochs, a learning rate of 1e-4, use mixed precision for training (more memory friendly). One could extend this with things like gradient accumulation and gradient checkpointing.\n",
        "\n",
        "I recommend [this guide](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one) which goes over all tips and tricks regarding maximizing fine-tuning performance on consumer hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2I7vJaDlvD2"
      },
      "outputs": [],
      "source": [
        "config = {\"max_epochs\": 2,\n",
        "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
        "          \"check_val_every_n_epoch\": 1,\n",
        "          \"gradient_clip_val\": 1.0,\n",
        "          \"accumulate_grad_batches\": 8,\n",
        "          \"lr\": 1e-4,\n",
        "          \"batch_size\": 1,\n",
        "          \"num_nodes\": 1,\n",
        "          \"warmup_steps\": 50,\n",
        "}\n",
        "# Instantiate yout module here\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-EncDAllvD2"
      },
      "source": [
        "## Define callbacks\n",
        "Optionally, Lightning allows to define so-called [callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html), which are arbitrary pieces of code that can be executed during training.\n",
        "\n",
        "You'd better use the EarlyStopping callback of Lightning, which will automatically stop training once the evaluation metric (edit distance in our case) doesn't improve after 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2KRo-rclvD2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
        "\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"VideoLLava-demo\")\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "class PushToHubCallback(Callback):\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
        "        pl_module.model.push_to_hub(REPO_ID,\n",
        "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
        "\n",
        "    def on_train_end(self, trainer, pl_module):\n",
        "        print(f\"Pushing model to the hub after training\")\n",
        "        pl_module.processor.push_to_hub(REPO_ID,\n",
        "                                    commit_message=f\"Training done\")\n",
        "        pl_module.model.push_to_hub(REPO_ID,\n",
        "                                    commit_message=f\"Training done\")\n",
        "\n",
        "early_stop_callback = EarlyStopping(monitor=\"your-metric-here\",\n",
        "                                    patience=3, verbose=False, mode=\"max\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ylKSrMlvD2"
      },
      "source": [
        "## Train!\n",
        " Trainer class supports many more flags. See the [docs](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liZhW7NhlvD3"
      },
      "outputs": [],
      "source": [
        "trainer = L.Trainer(\n",
        "        # YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1XGM6QflvD3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "trainer.fit(model_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbEdmdrn_K_l"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQPiR5IV89qs"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's see if the model has learned something. First load the model from the hub first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc0JHzIY89qs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lqxte3U89qs"
      },
      "source": [
        "See one example from the validation set here and plot 8 frames to see what is happening in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNw1jE0n89qw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "prompt, clip = eval_dataset[2]\n",
        "fig, axarr = plt.subplots(1, 2, figsize = (10, 10))\n",
        "fig.tight_layout()\n",
        "\n",
        "for i in range(2):\n",
        "    curr_frame = Image.fromarray(np.uint8(clip[i]))\n",
        "    axarr[i].imshow(curr_frame)\n",
        "    axarr[i].get_xaxis().set_visible(False)\n",
        "    axarr[i].get_yaxis().set_visible(False)\n",
        "    axarr[i].set_aspect('equal')\n",
        "\n",
        "plt.subplots_adjust(wspace=None, hspace=None)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWWHU1xB89qx"
      },
      "source": [
        "Next you need to prepare the video for the model, along with the text prompt we used during training. You need to apply the chat template to make sure the format is respected.\n",
        "\n",
        "Notice that this is exactly the same as what you did for the evaluation data collate function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBWCEdiT89qx"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Brzvy689qx"
      },
      "source": [
        "Next you should let the model autoregressively generate tokens using the [generate()](https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method, which is recommended for use at inference time. This method feeds each predicted token back into the model as conditioning for each next time step.\n",
        "\n",
        "Do note that there are various ways of decoding text, here we use greedy decoding which is the default. There are various fancier methods such as beam search and top-k sampling. Refer to [this amazing blog post](https://huggingface.co/blog/how-to-generate) for all details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFSVXwN889qx"
      },
      "outputs": [],
      "source": [
        "# Generate token IDs\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Decode back into text\n",
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c44886c523194321a45ac116a99d7e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c53a11f8b4d48bab7bb07005e09dab2",
              "IPY_MODEL_1e40a2be14574c9cab05c6a2066506c9",
              "IPY_MODEL_6997d1b4ec2e40268bbfe56ff5ad1ddc"
            ],
            "layout": "IPY_MODEL_0c80b263628d4c37b129278c8cb80fc2"
          }
        },
        "4c53a11f8b4d48bab7bb07005e09dab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_606c508016d34f1bb86d21b2c4e3cfc3",
            "placeholder": "​",
            "style": "IPY_MODEL_8c9c50dc1265410e82248451e1fdd156",
            "value": "Map: 100%"
          }
        },
        "1e40a2be14574c9cab05c6a2066506c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9e922c6bf14487599e94963b8afd747",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7dd46cf21684684b12361c82c5e01ca",
            "value": 188
          }
        },
        "6997d1b4ec2e40268bbfe56ff5ad1ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c13ed62e28e44468633fdfa010a07cb",
            "placeholder": "​",
            "style": "IPY_MODEL_24b4a2b91c7e484bacb5a543a60dff44",
            "value": " 188/188 [00:00&lt;00:00, 1990.77 examples/s]"
          }
        },
        "0c80b263628d4c37b129278c8cb80fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "606c508016d34f1bb86d21b2c4e3cfc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9c50dc1265410e82248451e1fdd156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9e922c6bf14487599e94963b8afd747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7dd46cf21684684b12361c82c5e01ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c13ed62e28e44468633fdfa010a07cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b4a2b91c7e484bacb5a543a60dff44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}